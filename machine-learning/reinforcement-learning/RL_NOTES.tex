\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{float} % To use 'H' for figures
\usepackage{graphicx} % to display images
\usepackage[dvipsnames]{xcolor} % to use colours
\usepackage{amssymb} % For 'mathbb'

\usepackage[normalem]{ulem} % underlying that breaks at line end

\usepackage{amsmath} % for advanced math formatting
\DeclareMathOperator*{\argmax}{arg\,max} % declare \argmax
\usepackage{amssymb} % for advanced math formatting

\usepackage{titlesec} % Title/Section Styling

\let\stdsection\section
\renewcommand\section{\newpage\stdsection} % Start each section from new page

\titleformat{\section} % uline sections
  {\normalfont\Large\bfseries}{\thesection}{1em}{}[{\titlerule[0.8pt]}]

\usepackage{tcolorbox} % for creating boxes

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\makeatletter
\newcommand\subsubsubsection{\@startsection{paragraph}{4}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\newcommand\subsubsubsubsection{\@startsection{subparagraph}{5}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\makeatother

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\title{Reinforcement Learning NOTES}
\author{Kristian Bonnici}


\begin{document}



\maketitle
\tableofcontents

\newpage
\subsection{Summary of Notation}\label{summary-of-notation}

\(\dot{=}\) is used for \textbf{``is defined as''}

\(S =\) set of \textbf{nonterminal states} * \(s, s' \rightarrow\) some
states

\(S^+ =\) set of \textbf{all states}, including the terminal state

\(A =\) set of \textbf{actions}

\(R =\) set of \textbf{rewards}

We'll assume that each of these sets will have a \emph{finite number of
elements}.

\(T \rightarrow\) transition function

\(V, V_t\) \(\rightarrow\) array \textbf{estimates} of
\(v_\pi(s)\) or \(v_*(s)\)

\(v_*(s) \rightarrow\) value of state s under the optimal policy
\begin{itemize}
  \item $= \max_\pi v_\pi(s)$
\end{itemize}

\(Q, Q_t\) \(\rightarrow\) array \textbf{estimates} of
\(q_\pi(s,a)\) or \(q_*(s,a)\)



\textcolor{blue}{\textbf{Return $G$:}} is the total of rewards

\begin{itemize}
  \item \textbf{$G$ for Episodic Tasks:} The return at time step $t$ is the \uline{sum of rewards until termination}.
  $$
  G_t \dot{=} R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T
  $$
  \item \textbf{$G$ for Continuing Tasks:} \uline{sum of discounted future rewards} (made to be always finite).
  \begin{align*}
    G_t &\dot{=} R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... + R_{t+k} + ... \\
    &\dot{=} \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
  \end{align*}
  \begin{itemize}
    \item \textbf{(recursively):}
    $$
    G_t \dot{=} R_{t+1} + \gamma G_{t+1}
    $$
  \end{itemize}
  \item Where:
  \begin{itemize}
    \item
      \(\gamma =\) is a parameter, \(0 \leq \gamma \leq 1\), called the
      \textbf{discount rate}.
      \begin{itemize}
        \item If $\gamma = 1$, undiscounted.
      \end{itemize}
  \end{itemize}
\end{itemize}





\textcolor{blue}{\textbf{Value functions:}}

\begin{itemize}
  \item The current time-step's \uline{state/action values} can be written \textbf{recursively} in terms of \textbf{future state/action values} with \textcolor{blue}{\textbf{Bellman equations}}.
\end{itemize}

\begin{itemize}
  \item \textcolor{blue}{\textbf{State-Value functions:}} \uline{expected return $G$ from a given state $s$}, following policy $\pi$.
  $$
  v_\pi (s) = \mathbb{E}_\pi[G_t|S_t=s]
  $$
  \begin{itemize}
  \item \textbf{(recursively):}
    $$
    v_\pi(s) = \mathbb{E}_\pi[\underbrace{R_{t+1}}_\text{immediate reward} + \underbrace{\gamma G_{t+1}}_\text{discounted return at time $t + 1$}|S_t=s]
    $$
  \item \textbf{(State-Value Bellman equation):}
    \begin{align*}
    \textcolor[HTML]{8F5760}{v_\pi(s)} &\dot{=} \textcolor[HTML]{8F5760}{\mathbb{E}_\pi[G_t|S_t=s]} \\
    &= \sum_a \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \textcolor[HTML]{8F5760}{\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']}] \\
    &= \underbrace{\sum_a \pi(a|s)}_\text{sum over possible action choices } \underbrace{\sum_{s'} \sum_{r} p(s',r|s,a)}_\text{ sum over possible rewards and next states} [r + \gamma \textcolor[HTML]{8F5760}{v_\pi(s')}]
    \end{align*}
    \begin{itemize}
      \item The result is a \uline{weighted sum} of terms consisting of immediate reward plus expected future returns from the next state $s'$.
    \end{itemize}

    \item \textbf{(State-Value Bellman Optimality equation):}
      $$
      v_*(s) = \max_a \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma v_*(s')]
      $$

  \item The magic of value functions is that we can use them as a stand-in for the average of an infinite number of possible futures.
  \end{itemize}
\end{itemize}



\begin{itemize}
  \item
  \textcolor{blue}{\textbf{Action-Value functions:}} \uline{expected return $G$ if the agent selects action $a$ in state $s$} and then follows policy $\pi$ thereafter.
    $$
    q_\pi(s,a) = \mathbb{E}_\pi[G_t | S_t=s, A_t = a]
    $$
    \begin{itemize}
      \item \textbf{(Action-Value Bellman equation):}
        \begin{align*}
        \textcolor[HTML]{8F5760}{q_\pi(s, a)} &\dot{=} \textcolor[HTML]{8F5760}{\mathbb{E}_\pi[G_t | S_t=s, A_t = a]} \\
        &= \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \sum_{a'} \pi(a'|s') \textcolor[HTML]{8F5760}{\mathbb{E}_\pi[G_{t+1} | S_{t+1}=s', A_{t+1} = a']}] \\
        &= \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \sum_{a'} \pi(a'|s') \textcolor[HTML]{8F5760}{q_\pi(s', a')}]
        \end{align*}

        \item \textbf{(Action-Value Bellman Optimality equation):}
          $$
          q_*(s, a) = \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \max_{a'} q_*(s', a')]
          $$
    \end{itemize}
\end{itemize}





\textcolor{blue}{\textbf{Policy \(\pi\):}} mapping from state to action (decision-making rule)

\begin{itemize}
  \item \textbf{Deterministic policy notation:} $\pi(s)=a$
  \item \textbf{Stochastic policy notation:} $\pi(a|s)$
  \item \textbf{Optimal Policy \(\pi_*\)}
  \begin{align*}
    \text{for } q_*(s,a) \rightarrow \pi_*(s) &= \argmax_a q_*(s,a) \\
    \text{for } v_*(s) \rightarrow\ \pi_*(s) &= \argmax_a E_{s'}[r(s,a,s') + \gamma v_*(s')] \\
    &=  \argmax_a \sum_{s'} T(s,a,s') (r(s,a,s') + \gamma v_*(s'))
  \end{align*}

  \begin{itemize}
    \item With Bellman equation (compare to State/Action-Value Bellman Optimality equation):
    \begin{align*}
    \pi_*(s) &= \argmax_a \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma v_*(s')] \\
    \pi_*(s) &= \argmax_a q_*(s,a)
    \end{align*}
  \end{itemize}
\end{itemize}

\begin{itemize}
\item
  \textbf{Value iteration:}, for estimating
  \(\pi \approx \pi_* \rightarrow \texttt{converges to } v_*(s)\). Only
  one iteration of iterative policy evaluation is performed between each
  step of policy improvement.

  \begin{itemize}
  \item
    Starting from \(V_0^*(s)=0\) for all (\(\forall\)) \(s\)
    \(\rightarrow\) iterate until convergence (usually change being
    smaller than some threshold we choose):

    \begin{itemize}
    \item
      \(V_{i+1}^*(s) = \max_a \sum_{s'} T(s,a,s') (r(s,a,s') + \gamma V_i^*(s'))\)
    \item
      \(\;\;\;\;\;\;\;\;\;\;\;\; = \max_a ImmediateReward + Discount*FutureRewards\)
    \end{itemize}
  \end{itemize}
\item
  \textbf{Policy iteration (iterative policy evaluation):}, for
  estimating \(\pi \approx \pi_*\).
\end{itemize}














\section{Introduction to Reinforcement Learning}\label{introduction-to-reinforcement-learning}

\textbf{Reinforcement learning} an area of machine learning concerned with how intelligent \textbf{agents} ought to take \textbf{actions} in an \textbf{environment} in order to maximize the notion of cumulative \textbf{reward}.

The \textbf{environment} is typically stated in the form of a Markov
decision process (MDP), because many reinforcement learning algorithms
for this context use dynamic programming techniques.

\textbf{Exploration \& Exploitation trade-off:} \textbf{Dilemma:}
choosing when to explore \& when to exploit?

\begin{itemize}
  \item \textbf{Exploration:} improve knowledge for long-term benefit.
  \item \textbf{Exploitation:} exploit knowledge for short-term benefit.
\end{itemize}

\textbf{Four main subelements of a reinforcement learning system:} a \textbf{policy}, a \textbf{reward signal}, a \textbf{value function}, and, optionally, a \textbf{model} of the environment.

\begin{itemize}
  \item \textbf{Policy} defines the learning agent's way of behaving at a given time.
  \item \textbf{Reward signal} indicates what is good in an immediate sense.
  \item \textbf{Value function} specifies what is good in the long run. Roughly speaking, the value of a state is the \emph{total amount of reward} an agent can expect to accumulate over the future, starting from that state.
  \item \textbf{Model} mimics the behaviour of the environment, or more generally, that allows inferences to be made about how the environment will behave.
  \begin{itemize}
    \item \textbf{Used for planning}, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced.
  \end{itemize}
\end{itemize}



\textbf{Model-based vs Model-free RL:}

\begin{itemize}
  \item If model \(\rightarrow\) \textbf{model-based} methods
  \item If no model \(\rightarrow\) simpler \textbf{model-free} methods
  \begin{itemize}
    \item explicitly trial-and-error learners---viewed as almost the opposite of planning.
  \end{itemize}
  \item \(\rightarrow\) \textbf{Modern reinforcement learning} spans the spectrum \textbf{from} low-level, trial-and-error learning \textbf{to} high-level, deliberative planning.
\end{itemize}


\textbf{Evolutionary methods:} (not focused on this course)

\begin{itemize}
  \item Instead of estimating value functions, these methods apply multiple static policies each interacting over an extended period of time with a separate instance of the environment. The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats.
  \item Although evolution and learning share many features and naturally work together, we do not consider evolutionary methods by themselves to be especially well suited to reinforcement learning problems and, accordingly, we do not cover them
  in this book.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}








\newpage
\part{Tabular Solution methods}\label{tabular-solution-methods}


In this part we describe almost all the core ideas of reinforcement
learning algorithms in their \textbf{simplest forms:} \(\rightarrow\)
State \& action spaces are small enough for the \textbf{approximate
value functions} to be represented as arrays, or tables.

\begin{itemize}
  \item \uline{Often find exact solutions}, that is, they can often find exactly the optimal value function and the optimal policy
\end{itemize}

This \textcolor{orange}{\textbf{contrasts with}} the \emph{approximate methods} described in the next part, which only find \uline{approximate solutions}, but which in return \textcolor{Green}{can be applied effectively to much larger problems}.

\textbf{Chapters in this section:}

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \item
    \textbf{Multi-armed Bandits:} special case of the reinforcement
    learning problem in which there is only a single state.
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{2}
  \item
    \textbf{Finite Markov Decision Processes:} the general problem
    formulation that we treat throughout the rest of the notes.

    \begin{itemize}
    \item
      Its main ideas including \textbf{Bellman equations} and
      \textbf{value functions}.
    \end{itemize}
  \end{enumerate}
\item
  The next three chapters (4., 5. \& 6.) describe \textbf{three
  fundamental classes of methods for solving finite Markov decision
  problems:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{3}
  \item
    \textbf{Dynamic Programming}

    \begin{itemize}
    \item
      \textbf{+} Well developed mathematically
    \item
      \textbf{-} Require a complete and accurate model of the
      environment
    \end{itemize}
  \item
    \textbf{Monte Carlo Methods}

    \begin{itemize}
    \item
      \textbf{+} Don't require a model
    \item
      \textbf{+} Conceptually simple
    \item
      \textbf{-} Not well suited for step-by-step incremental
      computation
    \end{itemize}
  \item
    \textbf{Temporal-Difference Learning}

    \begin{itemize}
    \item
      \textbf{+} Don't require a model
    \item
      \textbf{+} Fully incremental
    \item
      \textbf{-} More complex to analyze
    \end{itemize}
  \end{enumerate}
\end{itemize}

~ ~ ~ ~ ~ The methods also differ in several ways with respect to their
efficiency and speed of convergence.

\begin{itemize}
\item
  The remaining two chapters (7. \& 8.) \textbf{describe how these three
  classes of methods can be combined} to obtain the best features of
  each of them.

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{6}
  \item
    \emph{n}-step Bootstrapping: Strengths of Monte Carlo methods can be
    \textbf{combined} with the strengths of temporal-difference methods
    via multi-step bootstrapping methods.
  \item
    \textbf{Planning and Learning with Tabular Methods:}
    temporal-difference learning methods can be \textbf{combined with}
    model learning and planning methods (such as dynamic programming)
    for a \textbf{complete and unified solution to the tabular
    reinforcement learning problem}.
  \end{enumerate}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}
\newpage










\section{Multi-armed Bandits}\label{multi-armed-bandits}

The most \textbf{important feature distinguishing reinforcement
learning} from other types of learning is that it uses training
information that \uline{\emph{evaluates} the actions taken} rather than instructs by giving correct actions. \(\rightarrow\) need for active exploration, for an explicit search for good behaviour.

\textbf{In this chapter} we \uline{\textbf{study} the evaluative aspect of reinforcement learning \textbf{in} a simplified setting}, one that does not involve learning to act in more than one situation.

Studying this case enables us to \textbf{see} most clearly \textbf{how}
evaluative feedback \textbf{differs from}, and yet \textbf{can be
combined with}, instructive feedback.



\subsection{A k-armed Bandit Problem }\label{k-armed-bandit-problem}

\textbf{Problem statement:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Being faced repeatedly with a \uline{choice among \(k\) different
  options/actions}
\item
  \uline{Receive numerical reward} chosen from a stationary probability
  distribution that depends on the action you selected

  \begin{itemize}
  \item
    Each of the \(k\) actions has an expected (mean) reward
    \(\rightarrow\) \textbf{value} of that action:

    \begin{itemize}
    \item
      \(q_*(a) = E[R_t | A_t = a]\)
    \end{itemize}
  \item
    We denote the \textbf{estimated value} as:

    \begin{itemize}
    \item
      \(Q_t(a)\)
    \end{itemize}
  \end{itemize}
\end{enumerate}


\begin{itemize}
\item
  We assume that \uline{you don't know the \textbf{action values} with
  certainty}, although you may have estimates. \textcolor{orange}{Otherwise it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value}
\end{itemize}

\textbf{Objective:} maximize the expected total reward over some time
period.

\textbf{Exploiting vs Exploring}



\begin{itemize}
  \item \textbf{Exploiting} your current knowledge:
  \begin{itemize}
    \item Selecting action whose estimated value is greatest
   (\emph{greedy} action). * \textbf{Goal:} maximize the expected reward on the one step
  \end{itemize}
  \item \textbf{Exploring} to improve your estimate of the nongreedy action's value:
  \begin{itemize}
  \item
    Not selecting \emph{greedy} action
  \item
    \textbf{Goal:} to produce the greater total reward in the long run
  \end{itemize}
\item
  There are many sophisticated methods for balancing
  \textbf{exploration} and \textbf{exploitation} for particular
  mathematical formulations of the \(k\)-armed bandit and related
  problems.

  \begin{itemize}
  \item
    most of these methods make strong assumptions about stationarity and
    prior knowledge that are \textbf{either} violated or impossible to
    verify in applications.
  \end{itemize}
\end{itemize}






\subsection{Action-value Methods}\label{action-value-methods}

\textbf{What:} methods for (1) \uline{estimating the \textbf{values} of actions} and for using the estimates to (2) \uline{make \textbf{action} selection decisions}.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  estimating the \textbf{values}
\end{enumerate}

\begin{itemize}
\item
  \textbf{ONE natural WAY} for \textbf{estimating the true value of an
  action} is by averaging the rewards actually received
  \textbf{(\emph{sample-average} method)}:
\end{itemize}

\begin{gather*}
Q_t(a) \dot{=} \frac{\textrm{sum of rewards when } a \textrm{ taken prior to } t}{\textrm{number of times } a \textrm{ taken prior to } t} = \frac{\sum_{i=1}^{t-1} R_i \cdot 1_{predicate, \mathbb{A}_t = a}}{\sum_{i=1}^{t-1} 1_{predicate, \mathbb{A}_t = a}} \\ \\
  \textbf{Where:} \\
  1_{predicate} \text{: denotes the random variable that is} \\
  \text{\textbf{1} if predicate is true and} \\
  \text{\textbf{0} if it is not.} \\
\end{gather*}


\begin{itemize}

\item
  If the denominator:
  \begin{itemize}
  \item
    \(= 0\), then we instead define \(Q_t(a)\) as some default value
    (e.g.~zero).
  \item
    \(\rightarrow \infty\), then by the \textbf{law of large numbers,
    \(Q_t(a)\) converges to \(q_*(a)\)}.
  \end{itemize}
\item
  \textbf{NOTE:} \textbf{\(Q_n\)} can be \textbf{computed in} a
  computationally efficient manner, in particular, with constant memory
  and constant per-time-step computation as:
\end{itemize}

\begin{gather*}
  Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n] \\
  NewEstimate = OldEstimate + StepSize[Target - OldEstimate]
\end{gather*}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\item
  make \textbf{action} selection decisions
\end{enumerate}

\begin{itemize}
\item
  The \textbf{simplest action selection rule}, is always selecting one
  of the actions with the highest estimated value \textbf{(\emph{greedy}
  action selection method)}:
\end{itemize}

\[A_t = \argmax_{a}Q_t(a)\]

\begin{itemize}
\item
  A \textbf{simplest alternative action selection rule}, is to behave
  greedily most of the time, but every once in a while, say with small
  probability \(\epsilon\), instead select action randomly
  \textbf{(\(\epsilon\)-greedy methods)}.
\end{itemize}







\subsection{Bandit Algorithm Examples }\label{bandit-algorithm-examples}

\textbf{Example:} 10-bandit problems

\begin{itemize}
\item Given:
\end{itemize}

\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.9\columnwidth]{images/10-armed-eg1.png}
    \caption{10-armed testbed}
    \label{fig:fig1}
\end{figure}

\begin{itemize}
\item
  \textbf{Figure \ref{fig:fig1}:} An example bandit problem from the 10-armed
  testbed. The true value \(q_*(a)\) of each of the ten actions was
  selected according to a normal distribution with mean zero and unit
  variance, and then the actual rewards were selected according to a
  mean \(q_*(a)\) unit variance normal distribution, as suggested by
  these gray distributions.
\end{itemize}

\begin{itemize}
\item
  Then compares a greedy method with two \(\epsilon\)-greedy methods
  (\(\epsilon = 0.01\) and \(\epsilon = 0.1\)):
\end{itemize}


\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.9\columnwidth]{images/10-armed-eg2.png}
    \caption{Average performance of \(\epsilon\)-greedy action-value methods on the 10-armed testbed}
    \label{fig:fig2}
\end{figure}


\begin{itemize}
\item
  \textbf{Figure \ref{fig:fig2}:} Average performance of \(\epsilon\)-greedy
  action-value methods on the 10-armed testbed. These data are averages
  over 2000 runs with different bandit problems. All methods used sample
  averages as their action-value estimates.
\end{itemize}

\begin{itemize}
\item
  NOTE:

  \begin{itemize}
  \item
    The \(\epsilon = 0.01\) method improved more slowly, but eventually
    would perform better than the \(\epsilon = 0.1\) method on both
    performance measures shown in the figure.
  \item
    It is \textbf{also possible} to reduce \(\epsilon\) over time to try
    to get the best of both high and low \(\epsilon\) values.
  \item
    With \textbf{noisier rewards} it takes more exploration to find the
    optimal action.
  \end{itemize}
\end{itemize}

\textbf{Exploration} is beneficial even in the deterministic worlds
\textbf{if:}
\begin{itemize}
  \item \textbf{nonstationary} task, that is, the \textbf{true
  values of the actions changed over time} \(\rightarrow\) \textbf{agent's
  decision-making policy changes}.
  \item \textbf{nonstationary} is the case most commonly encountered in reinforcement learning.
\end{itemize}

\textbf{Example:} A simple bandit algorithm

Pseudocode for a complete bandit algorithm using incrementally computed
sample averages and \(\epsilon\)-greedy action selection is shown in the
box below.

\begin{tcolorbox}
  \textbf{Initialize, for \(a = 1 \; to \; k\):} \(\;\;\;\)
  \(Q(a) \leftarrow 0\) \(\;\;\;\) \(N(a) \leftarrow 0\)

  \textbf{Loop forever:}
  $$\;\;\; A \leftarrow \begin{cases} \argmax_{a}Q(a) &\texttt{with probability } 1- \epsilon \;\;\; \texttt{(breaking ties randomly)} \\ \texttt{random action} &\texttt{with probability } \epsilon \end{cases}$$
  $$\;\;\; R \leftarrow bandit(A)$$
  $$\;\;\; N(a) \leftarrow N(a)+1$$
  $$\;\;\; Q(A) \leftarrow Q(A) + \frac{1}{N(A)}[R-Q(A)]$$
\end{tcolorbox}

\begin{itemize}
\item
  Where:

  \begin{itemize}
  \item
    \textbf{function \(bandit(a)\)} is assumed to \textbf{take} an
    action and \textbf{return} a corresponding reward
  \end{itemize}
\end{itemize}









\subsection{Tracking a Nonstationary Problem}\label{tracking-nonstationary-problem}

\textbf{What:} true values of the actions changed over time
\(\rightarrow\) agent's decision-making policy changes.

\textbf{Adjustments:} it makes sense to give more weight to recent
rewards than to long-past rewards.

\begin{itemize}
\item
  A \textbf{POPULAR WAY} it to use a \textbf{constant step-size
  parameter \(\alpha\)} (2.3 modified to be):
\end{itemize}

\begin{gather*}
  Q_{n+1} = Q_n + \alpha [R_n - Q_n] \\ \\
  \textbf{Where:} \\
  \alpha \in (0,1]\ \text{: is constant} \\
\end{gather*}


\textbf{\(\rightarrow\) resulting in \(Q_{n+1}\) being a weighted
average of past rewards and the initial estimate \(Q_1\)} (sometimes
called an \emph{exponential recency-weighted average}):

\[Q_{n+1} = (1 - \alpha )^n Q_1 + \sum_{i=1}^n \alpha (1 - \alpha)^{n-1} R_i \;\;\;\;\;(2.6)\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}











\subsection{Optimistic Initial Values}\label{optimistic-initial-values}


\textbf{What:} a common strategy of balancing exploration-exploitation.

\textbf{How:} encouraging early exploration with optimistic initial values for all possible actions.

\textbf{Limitations:}
\begin{itemize}
  \item \textcolor{red}{Drive only early exploration}
  \item \textcolor{red}{Not well-suitable for} non-stationary problems
  \item \textcolor{red}{We may not always know how to set the optimistic initial values}, because in practice we may not know the maximal reward.
\end{itemize}














% =============== CHAPTER 3 - Finite Markov Decision Processes ===============



\section{Finite Markov Decision Processes}\label{finite-markov-decision-processes}

\textbf{Markov decision process (MDP)} provides a \uline{mathematical framework
for modeling sequential decision-making in situations where outcomes are partly
random and partly under the control of a decision maker}.

A \textbf{MDP is a 4-tuple \((S,A,p,R)\)}, where:

\begin{itemize}
\item
  \(S\) is a set of states called the state space,
\item
  \(A\) is a set of actions called the action space (alternatively,
  \(A(s)\) is the set of actions available from state \(s\)),
\item
  \(p(s', r|s, a) = Pr\{S_t = s', R_t =r | S_{t-1} = s, A_{t-1} = a\}\)
  is the probability that action \(a\) in state \(s\) at time \(t\) will
  lead to state \(s'\) at time \(t+1\),
\item
  \(R_{a}(s,s')\) is the immediate reward (or expected immediate reward)
  received after transitioning from state \(s\) to state \(s'\), due to
  action \(a\)
\end{itemize}

The state and action spaces may be \textbf{finite} or \textbf{infinite}:

\begin{itemize}
  \item e.g. the set of real numbers is infinite.
  \item Some processes with
  countably infinite state and action spaces can be reduced to ones with
  finite state and action spaces.
\end{itemize}

A \textbf{policy function \(\pi\)} is a (potentially probabilistic)
mapping from state space to action space.

\textbf{Optimization objective} \(\rightarrow\) find a good ``policy''
for the decision maker.

\begin{itemize}
  \item \textbf{EXTRA:} Once a MDP is \textbf{combined
  with} a policy in this way, this fixes the action for each state and the
  resulting combination \textbf{behaves like a Markov chain} (since the
  action chosen in state \(s\) is completely determined by \(\pi(s)\) and
  \(\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)\) reduces to
  \(\Pr(s_{t+1}=s'\mid s_{t}=s)\), a \textbf{Markov transition matrix}).
\end{itemize}






\subsection{The Agent--Environment Interface }\label{agent-environment-interface}

\textbf{Agent:} learner and decision maker.

\textbf{Environment:} thing agent interacts with, comprising everything
outside the agent.

The environment also gives rise to \textbf{rewards}, special numerical
values that the agent seeks to maximize over time through its choice of
\textbf{actions}.


\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.9\columnwidth]{images/MDP-agent-environment-interaction.png}
    \caption{MDP agent-environment interaction}
    \label{fig:fig3}
\end{figure}


The MDP and agent together give rise to a \emph{sequence} or
\emph{trajectory} that begins like this:

\[S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,... \]

\emph{In general}, \textbf{actions} can be any decisions we want to
learn how to make, and the \textbf{states} can be anything we can know
that might be useful in making them.

In a \textbf{finite MDP}, the sets of states, actions, and rewards
(\(S\), \(A\), and \(R\)) all have a finite number of elements.

In this case, the \textbf{random variables \(R_t\) and \(S_t\)} have
well defined discrete probability distributions \textbf{dependent only
on} the preceding state and action.

There is a \textbf{probability of those values occurring at time t},
given particular values of the preceding state and action:

\[p(s', r|s, a) = Pr\{S_t = s', R_t =r | S_{t-1} = s, A_{t-1} = a\},\]

Where:

\begin{itemize}
\item
  \(s' =\) particular values of the random variable \(S\) (\(s' \in S\))
\item
  \(r =\) particular values of the random variable \(R\) (\(r \in R\))
\end{itemize}

The \textbf{function \(p\)} defines the dynamics of the MDP. \(p\)
specifies a probability distribution for each choice of \(s\) and \(a\).

Total probability is thus:

\[\sum_{s' \in S} \sum_{r \in R} p(s', r|s, a) = 1, for\;all\;s \in S, a \in A(s)\]

\textbf{Markov property:} The state must include information about all
aspects of the past agent--environment interaction that make a
difference for the future.

\begin{itemize}
  \item (only present matters)
  \item (things/rules/transition model are stationary)
  \item We will assume the
  Markov property throughout this book.
\end{itemize}


\textbf{Calculations from the four-argument dynamics function:}

\begin{itemize}
\item
  \textbf{state-transition probabilities
  \(p : S \times S \times A \rightarrow [0, 1]\)}

  \begin{itemize}
  \item
    \(p(s'|s, a) = Pr\{S_t = s'| S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in R} p(s', r|s, a)\)
  \end{itemize}
\item
  \textbf{expected rewards for state--action pairs
  \(r : S \times A \rightarrow \mathbb{R}\)}

  \begin{itemize}
  \item
    \(r(s,a) = E[R_t | S_{t-1}=s, A_{t-1}=a] = \sum_{r \in R} \sum_{s' \in S} p(s', r|s, a)\)
  \end{itemize}
\item
  \textbf{expected rewards for state--action--next-state triples
  \(r : S \times A \times S \rightarrow \mathbb{R}\)}

  \begin{itemize}
  \item
    \(r(s,a, s') = E[R_t | S_{t-1}=s, A_{t-1}=a, S_t = s'] = \sum_{r \in R} r \frac{p(s', r | s, a)}{p(s' | s, a)}\)
  \end{itemize}
\end{itemize}









\subsection{Goals and Rewards}\label{goals-and-rewards}

\textbf{Reward Hypothesis:}

\begin{itemize}
\item
  That all of what we mean by \textbf{goals $G$} and purposes \emph{can be well thought of as} the \uline{maximization of the expected value of the cumulative sum of a received scalar signal (called reward)}.
\end{itemize}

It is critical that the rewards we set up truly indicate what we want
accomplished.

\begin{itemize}
  \item \textbf{For example}, a chess-playing agent should be
  rewarded only for actually winning, not for achieving subgoals such as
  taking its opponent's pieces or gaining control of the center of the
  board. If achieving these sorts of subgoals were rewarded, then the
  agent might find a way to achieve them without achieving the real goal.
\end{itemize}

\textbf{Reward signal} is your way of communicating to the robot what
you want it to achieve, not how you want it achieved.







\subsection{Returns and Episodes }\label{returns-and-episodes}

In general, we seek to maximize the expected \textbf{return}, where the
return, denoted \(G_t\).

\textbf{Episodic Tasks}\label{episodic-tasks}

\(G_t\) is in the simplest case the \textbf{return} of the sum of the
rewards:

\[G_t \dot{=} R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T \;\;\;\;\;(3.7)\]

Tasks with \emph{episodes} of this kind are called \textbf{episodic
tasks}. In episodic tasks we \textbf{sometimes need to distinguish} the
set of all nonterminal states, denoted \(S\), from the set of all states
plus the terminal state, denoted \(S^+\). The time of termination,
\(T\), is a random variable that normally varies from episode to
episode.


\textbf{Continuing Tasks}\label{continuing-tasks}


On the other hand, in many cases the agent--environment interaction does
not break naturally into identifiable episodes, but goes on
\textbf{continually without limit}. We call these \textbf{continuing
tasks}.

In this book we usually use a definition of return that is slightly more
complex conceptually but much simpler mathematically. The additional
concept that we need is that of \textbf{discounting}. According to this
approach, the agent tries to select actions so that the sum of the
discounted rewards it receives over the future is maximized. In
particular, it chooses \(A_t\) to maximize the expected discounted
return:

\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \;\;\;\;\;(3.8)\]

Where:

\begin{itemize}
\item
  \(\gamma =\) is a parameter, \(0 \leq \gamma \leq 1\), called the
  \textbf{discount rate}.

  \begin{itemize}
  \item
    If \(\gamma < 1\), the infinite sum has a finite value as long as
    the reward sequence \(\{R_k\}\) is bounded.
  \item
    If \(\gamma = 0\), the agent is ``myopic'' in being concerned only
    with maximizing immediate rewards: its objective in this case is to
    learn how to choose \(A_t\) so as to maximize only \(R_{t+1}\). But
    in general, acting to maximize immediate reward can reduce access to
    future rewards so that the return is reduced.
  \item
    As \(\gamma\) approaches \(1\), the return objective takes future
    rewards into account more strongly; the agent becomes more
    farsighted.
    \begin{itemize}
      \item If $\gamma = 1$, undiscounted.
    \end{itemize}
  \end{itemize}
\end{itemize}

\textbf{Returns at successive time steps} are related to each other in a
way that is important for the theory and algorithms of reinforcement
learning:

\begin{align*}
  G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... \\
  &= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ...)  \\
  &= R_{t+1} + \gamma G_{t+1} \\
\end{align*}

\begin{itemize}
\item
  Note that this works for all time steps \(t<T\), even if termination
  occurs at \(t + 1\), if we define \(G_T = 0\)
\item
  Note that although the return is a sum of an infinite number of terms,
  it is still finite if the reward is nonzero and constant, if
  \(\gamma < 1\).
\end{itemize}






\subsection{Unified Notation for Episodic and Continuing Tasks}
\label{unified-notation-for-episodic-and-continuing-tasks}

In the preceding section we described two kinds of reinforcement
learning tasks:

\begin{itemize}
\item
  \textbf{episodic tasks:} agent--environment interaction naturally
  breaks down into a sequence of separate episodes.
\item
  \textbf{continuing tasks:} agent--environment interaction don't breaks
  down into a sequence of separate episodes.
\end{itemize}

We have defined the return as a sum over a finite number of terms in one
case (3.7) and as a sum over an infinite number of terms in the other
(3.8). These two can be unified by considering \textbf{episode
termination} to be the entering of a \uline{special absorbing state that
transitions only to itself and that generates only rewards of zero}. For
example, consider the state transition diagram:


\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.9\columnwidth]{images/Unified-Notation-Episodic-and-Continuing-Tasks.png}
    \caption{State transition diagram}
    \label{fig:fig4}
\end{figure}


Starting from \(S_0\), we get the reward sequence
\(+1, +1, +1, 0, 0, 0,...\). Summing these, we get the same return
whether we sum over the first \(T\) rewards (here \(T = 3\)) or over the
full infinite sequence. This remains true even if we introduce
discounting.

Thus, we can define the \textbf{return, in general}, according to (3.8),
using the convention of omitting episode numbers when they are not
needed, and including the possibility that = 1 if the sum remains
defined (e.g., because all episodes terminate). Alternatively, we can
write:

\[G_t = \sum_{k=t+1}^T \gamma^{k-t-1} R_k\]

\begin{itemize}
\item
  including the possibility that \(T = \infty\) or \(\gamma = 1\) (but
  not both).
\end{itemize}







\subsection{Policies and Value Functions}\label{policies-and-value-functions}

Almost all \textbf{reinforcement learning algorithms involve estimating
\textcolor{blue}{value functions}} --- functions of states (or of state--action
pairs) that \uline{estimate how good it is for the agent to be in a given state} (or \uline{how good it is to perform a given action in a given state}). They allow an agent to query the quality of its current situation instead of waiting to observe the long-term outcome. Thus, \uline{\textbf{value functions} enable us to judge the quality of different \textbf{policies}}.

\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.8\columnwidth]{images/value-function-predicting-rewards.png}
    \caption{Illustration of value function predicting rewards into the future}
    \label{fig:value-function-predicting-rewards}
\end{figure}

\textcolor{blue}{\textbf{Policy \(\pi\)}} is a \uline{\textbf{mapping from} states \textbf{to} probabilities of selecting each possible action}. If the agent is
following policy \(\pi\) at time \(t\), then \(\pi(a|s)\) is the
probability that \(A_t = a\) if \(S_t = s\).

A policy by definition \uline{depends only on the current state}. It cannot depend on things like time or previous states. This is best thought of as a restriction on the state, not the agent. The state should provide the agent with all the information it needs to make a good decision.

\begin{itemize}
  \item Accordingly, value functions are defined with respect to particular ways of acting, called \textbf{policies}.
\end{itemize}

\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.8\columnwidth]{images/deterministic-and-stochastic-policies.png}
    \caption{Policies tell an agent how to behave in their environment}
    \label{fig:value-function-predicting-rewards}
\end{figure}

\textbf{Reinforcement learning methods specify} \uline{how the agent's policy is changed as a result of its experience}.

The \textcolor{blue}{\textbf{state-value functions \(v_\pi(s)\)}} of a state \(s\) under a policy \(\pi\), is the expected return when starting in \(s\) and following \(\pi\) thereafter. For MDPs, we can define \(v_\pi\) formally by

\[v_\pi(s) = E_\pi[G_t|S_t=s] = E_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s], for\;all\; s \in S \;\;\;\;\;(3.12)\]

Where:

\begin{itemize}
\item
  \(E_\pi[]\) denotes the expected value of a random variable given that
  the agent follows policy \(\pi\), and \(t\) is any time step.
\end{itemize}

The \textcolor{blue}{\textbf{action-value function \(q_\pi(s, a)\)}} for policy \(\pi\) is
defined as the \textbf{value of taking action \(a\) in state \(s\) under
a policy \(\pi\)}, as the expected return starting from \(s\), taking
the action \(a\), and thereafter following policy \(\pi\):

\[q_\pi(s, a) = E_\pi[G_t|S_t=s, A_t=a] = E_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a] \;\;\;\;\;(3.13)\]

A \textbf{fundamental property of value functions} used throughout
reinforcement learning and dynamic programming is that they satisfy
recursive relationships similar to that which we have already
established for the return (3.9).








\subsection{Optimal Policies and Value Functions}\label{optimal-policies-and-value-functions}

\textcolor{Green}{\textbf{Solving a reinforcement learning task}} means, roughly, finding a policy that achieves a lot of reward over the long run.

For finite MDPs, we can precisely define an \textcolor{blue}{\textbf{optimal policy \(\pi_*\)}} as follows: * \(\pi \geq \pi'\) if and only if
\(v_\pi(s) \geq v_{\pi'}(s)\) for all \(s \in S\) * \textbf{Always at
least one policy} that is better than or equal to all other policies.


\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.8\columnwidth]{images/optimal-policy-eg.png}
    \caption{Illustration of optimal policy}
    \label{fig:optimal-policy-eg}
\end{figure}

How to find an optimal policy:
\begin{itemize}
  \item Once we had the \uline{optimal state value function}, it's relatively easy to work out the optimal policy.
  \item If we have the \uline{optimal action value function}, working out the optimal policy is even easier.
\end{itemize}

All optimal policies share the same \textcolor{blue}{\textbf{optimal state-value function
\(v_*\)}} defined as:

\[v_*(s) = \max_\pi v_\pi(s) \;for\;all\; s \in S \;\;\;\;\;\;(3.15)\]

Optimal policies also share the same \textcolor{blue}{\textbf{optimal action-value function \(q_*\)}} defined as:

\[q_*(s, a) = \max_\pi q_\pi(s, a) \;for\;all\; s \in S \; \& \; a \in A(s) \;\;\;\;\;\;(3.16)\]

For the state--action pair \((s, a)\), this function gives the expected
return for taking action \(a\) in state \(s\) and thereafter following
an optimal policy. Thus, we can write \textbf{\(q_*\) in terms of
\(v_*\)} as follows \textbf{(Bellman equation)}:

\[q_*(s, a) = E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \;\;\;\;\;\;(3.17)\]






\subsection{Optimality and Approximation }\label{optimality-and-approximation}

For the kinds of tasks in which we are interested, \textcolor{red}{optimal policies can be generated only with extreme computational cost}. In particular, the amount of computation it can perform in a single time step.







\subsection{Why Bellman equations}\label{why-bellman-equations}

\textcolor{blue}{\textbf{Bellman equations}} define a \textbf{relationship between} the \uline{value of a state} or state-action pair and its \uline{possible successor states}.


Consider:

\begin{itemize}
  \item \textbf{Environment:}
  \begin{itemize}
    \item 4 states (A, B, C, D) in a square gridworld
    \item 25\% probability of moving (up, down, left or right)
    \begin{itemize}
      \item bumping into a border will result into staying in the current state
    \end{itemize}
  \end{itemize}
  \item \textbf{Policy:} Uniform random policy
  \item \textbf{Rewards:} The reward is 0 everywhere except for any time the agent lands in state B, the reward is +5.
  \item \textbf{Discoun factor $\gamma$:} 0.7
\end{itemize}

For the given setup, we can write value functions for each of the states (A, B, C, D) in form of Bellman equations:

\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.9\columnwidth]{images/gridworld-eg.png}
    \caption{Value functions for all gridworld states}
    \label{fig:gridworld-eg}
\end{figure}
\begin{itemize}
  \item \textbf{Unique solutions to the Bellman equations:}
  \begin{itemize}
    \item $V_\pi(A) = 4.2$
    \item $V_\pi(B) = 6.1$
    \item $V_\pi(C) = 2.2$
    \item $V_\pi(D) = 4.2$
  \end{itemize}
\end{itemize}

The \textbf{important thing to note} is that the \uline{Bellman equation reduced an unmanageable infinite sum over possible futures, to a simple linear algebra problem}.

In this case we used the \textbf{Bellman equation} \uline{to directly write down a system of equations for the state values, and then some the system to find the values}. This approach may be possible for MDPs of moderate size. However, \textcolor{Red}{in more complex problems, this won't always be practical}.

\begin{itemize}
  \item e.g. Consider the game of chess for example. We probably won't be able to even list all the possible states, there are around 10 to the 45 of them.

  \begin{figure}[H]
    \centering  % Remember to centre the figure
      \includegraphics[width=0.8\columnwidth]{images/gridworld-eg2.png}
      \caption{Illustration how directly solving MDPs gets quickly problematic}
      \label{fig:gridworld-eg}
  \end{figure}

\end{itemize}




\subsection{Summary}\label{summary}

\textcolor{Green}{\textbf{Solving a reinforcement learning task}} means, roughly, finding a policy that achieves a lot of reward over the long run.

The \textbf{first step in applying reinforcement learning} will always be to \uline{formulate the problem as an MDP}.

\textcolor{blue}{\textbf{Markov property:}} The state must include information about all aspects of the past agent--environment interaction that make a
difference for the future.
\begin{itemize}
  \item (only present matters)
  \item (things/rules/transition model are stationary)
  \item We will assume the
  Markov property throughout this book.
\end{itemize}

\textcolor{blue}{\textbf{Policy \(\pi\), \(\pi(a|s)\):}} action to take for any given
state

\begin{itemize}
  \item Any policy: \(\pi(s) \rightarrow a\)
  \item Optimal policy:
  \(\pi^*(s) \rightarrow a\) * maximizes long-term expected reward
\end{itemize}


\textcolor{blue}{\textbf{State-Value functions \(v_\pi(s)\):}} expected cumulative rewards
when starting in \(s\) and following policy \(\pi\) thereafter.

\begin{itemize}
  \item or MDPs, we can define \(v_\pi\) formally by
  $$v_\pi(s) = E_\pi[G_t|S_t=s]$$

  \item \textbf{State-Value functions \(v_\pi(s)\):} can be decomposed into \textbf{immediate} and \textbf{future} components using \textcolor{blue}{\textbf{Bellman equation}}. Bellman equation forms the basis of a number of ways to compute, approximate, and learn \(v_\pi\).
  \begin{align*}
  V(s) &= E[G_t | s_t = s] \\
  V(s) &= E[r_{t+1} + \gamma V(s_{t+1} | s_t = s)] \\
  \end{align*}
  \begin{gather*}
    \textbf{Where:} \\
    \gamma =: \text{ is a parameter, \(0 \leq \gamma \leq 1\), called the
    \textbf{discount rate}.} \\
    \text{When \(0\) \(\rightarrow\) consider only
    immediate rewards} \\
    \text{When approaches \(1 \rightarrow\) forward looking}
  \end{gather*}
\end{itemize}

\textcolor{blue}{\textbf{Action-Value functions \(q_\pi(s, a), Q_\pi\):}} expected cumulative reward of taking action \(a\) when starting in \(s\) and
following policy \(\pi\) thereafter.
$$q_\pi(a) = E[R_t | A_t = a]$$

\begin{itemize}
  \item \textbf{Action-Value functions \(v_\pi(s)\):} can also be decomposed
  into \textbf{immediate} and \textbf{future} components using
  \textcolor{blue}{\textbf{Bellman equation}}.
  $$Q_\pi(s,a) = E_\pi[r_t + \gamma Q_\pi (S_{t+1}, a_{t+1}) | s_t=s, a_t=a]$$
  $$Q_\pi(s,a) = \sum_{s'} T(s,a,s') r(s,a,s') + \gamma \sum_{s'}T(s,a,s')Q_\pi(s',\pi(s'))$$
\end{itemize}

\textcolor{blue}{\textbf{Return \(G\) \& Rewards \(R\):}} return is the total of rewards

\begin{itemize}
  \item \(R(s) =\) reward of \textbf{entering state \(s\)}
  \item \(R(s, a) =\) reward of \textbf{entering state \(s\)} \& \textbf{taking action \(a\)}
  \item \(R(s, a, s') =\) reward of \textbf{being in state \(s\)} \& \textbf{taking action \(a\)} \& \textbf{entering state \(s'\)}
\end{itemize}

\textcolor{blue}{\textbf{State \(S\):}} every state agent could be in

\begin{itemize}
  \item \(S =\) set of \textbf{nonterminal states}
  \begin{itemize}
    \item \(s' =\) particular values of the random variable \(S\) (\(s' \in S\))
  \end{itemize}
  \item \(S^+ =\) set of \textbf{terminal states}
\end{itemize}

\textcolor{blue}{\textbf{Actions \(A, A(s)\):}} every action agent could take

\textcolor{blue}{\textbf{Function \(p\) (a.k.a Model / Transition function):}} defines the
dynamics of the environment. \(p\) specifies a probability distribution
for each choice of \(s\) and \(a\).

\begin{itemize}
  \item \textbf{state-transition
  probabilities \(p : S \times S \times A \rightarrow [0, 1]\)}
  $$
  p(s'|s, a) = Pr\{S_t = s'| S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in R} p(s', r|s, a)
  $$
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}












% =============== CHAPTER 4 - Dynamic Programming ===============


\section{Dynamic Programming (DP)}\label{dynamic-programming}


\textbf{What:} refers to a \uline{collection of algorithms that can be used to }\textbf{\uline{compute optimal policies}}\uline{ given a perfect model of the environment as a Markov decision process (MDP)}.


Pros and cons:

\begin{itemize}
  \item \textbf{+} Well developed mathematically
  \begin{itemize}
    \item \textcolor{orange}{All methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.}
  \end{itemize}
  \item \textbf{-} Require a complete and accurate model of the environment (access to the dynamics function $p$)
\end{itemize}


DP algorithms \textbf{use the \textcolor{blue}{Bellman equations} to define iterative algorithms} for both \textcolor{blue}{\uline{policy evaluation}} and \textcolor{blue}{\uline{policy control}}.




\subsection{Policy Evaluation vs. Policy Control}\label{policy-evaluation-vs-policy-control}

\textbf{What:}
\begin{itemize}
  \item \textbf{\textcolor{blue}{Policy Evaluation:}} the task of \uline{determining the value function $v_\pi$ for a specific policy $\pi$}
  \item \textbf{\textcolor{blue}{Policy Control:}} the task of \uline{improving an existing policy $\pi$} (until it's optimal $\pi_*$).
\end{itemize}

\textbf{Why:}
\begin{itemize}
  \item \textbf{\textcolor{blue}{Policy Evaluation:}} for assessing how good a policy is $\rightarrow$ (to improve it)
  \item \textbf{\textcolor{blue}{Policy Control:}} for finding the optimal policy (goal of reinforcement learning)
\end{itemize}



\subsection{Iterative Policy Evaluation (Prediction)}\label{policy-evaluation-prediction}

\textbf{What:} \uline{\textbf{In \textcolor{blue}{DP}} we can do \textbf{policy evaluation}} to \uline{\textbf{\textcolor{blue}{iteratively}} improve value function}.


\textbf{How:} by turning \uline{\textbf{Bellman equation} into an \textbf{update rule}}:

  \begin{gather*}
    v_{\textcolor[HTML]{8F5760}{\pi}}(s) \textcolor[HTML]{8F5760}{=} \sum_a \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma v_{\textcolor[HTML]{8F5760}{\pi}}(s')] \\
    \Bigg\downarrow \\
    v_{\textcolor[HTML]{8F5760}{k+1}}(s) \textcolor[HTML]{8F5760}{\leftarrow} \sum_a \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma v_{\textcolor[HTML]{8F5760}{k}}(s')] \\
  \end{gather*}

\begin{itemize}
    \item This will \textbf{produce} a \textcolor{Green}{sequence of better and better approximations to the value function}.

    \item We begin with an \textbf{arbitrary initialization} for our approximate value function, let's call this $v_0$.
    \begin{itemize}
      \item For any $v_0$:
        $$
        \lim_{k \to \infty} v_k = v_\pi
        $$
    \end{itemize}
    \item To implement iterative policy evaluation, we \textbf{store two arrays}:
    $$
    \textcolor[HTML]{8F5760}{V'}(s) \leftarrow \sum_a \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \textcolor[HTML]{8F5760}{V} (s')]
    $$
    \begin{itemize}
      \item $\textcolor[HTML]{8F5760}{V} \; [ \;\; | \;\; | \;\; | \;\; ] \rightarrow$ stores the \uline{current approximate value function}
      \item $\textcolor[HTML]{8F5760}{V'} \; [ \;\; | \;\; | \;\; | \;\; ] \rightarrow$ stores the \uline{updated values}
      \begin{itemize}
        \item we can compute the new values from the old one state at a time without the old values being changed in the process.
        \item At the end of a full sweep, we can write all the new values into V; then we do the next iteration.
      \end{itemize}
      \item \textcolor{Orange}{It is also possible to implement a version with only one array, in which case, some updates will themselves use new values instead of old. This single array version is still guaranteed to converge, and in fact, will usually converge faster.}
    \end{itemize}
\end{itemize}


\begin{tcolorbox}[title={Iterative Policy Evaluation, for estimating $V \approx v_\pi$}]

  \textbf{Input:} $\pi$ (policy to be evaluated) \\

  \textbf{Algorithm parameters:}
  \begin{itemize}
    \item $\theta \leftarrow$ small threshold determining accuracy of estimation
  \end{itemize}

  \textbf{Initialize:}
  \begin{itemize}
    \item $V(s)$, for all $s \in \mathcal{S}^+$ arbitrarily
    \item $V'(s)$, for all $s \in \mathcal{S}^+$ arbitrarily
    \item $V(terminal) = 0$
  \end{itemize}


  \textbf{Loop:}
  \begin{description}
      \item $\;\;\;$ $\Delta \leftarrow 0$
      \item $\;\;\;$ \textbf{Loop for each $s \in \mathcal{S}$:}
      \begin{description}
        \item $\;\;\;$ $\textcolor[HTML]{8F5760}{V'}(s) \leftarrow \sum_a \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \textcolor[HTML]{8F5760}{V} (s')]$
        \item $\;\;\;$ $\Delta \leftarrow \max (\Delta , | V'(s) - V(s) |)$
      \end{description}
      \item $\;\;\;$ $V \leftarrow V'$
  \end{description}
  \textbf{Until:} $\Delta < \theta$ (a small positive number) \\

  \textbf{Output:} $V \approx v_\pi$
\end{tcolorbox}

\begin{itemize}
  \item We track the largest update $\Delta$ to each state value in a given iteration.
  \item The outer loop terminates when this maximum change $\Delta$ is less than some user-specified constant $\theta$.
\end{itemize}








\subsection{Policy Improvement}\label{policy-improvement}

\textbf{\textcolor{blue}{Policy improvement theorem:}} tells us that \uline{greedified policy is a strict improvement}, (unless the original policy was already optimal). In other words, it tells us that we can construct a strictly better policy by acting greedily with respect to the value function of a given policy, unless the given policy was already optimal.

Greedy policy:

$$
\pi'(s) =  \argmax_a \sum_{s'} \sum_r p(s',r | s,a)[r + \gamma v_\pi(s')]
$$


Better:
$$
q_{\textcolor[HTML]{8F5760}{\pi}}(s,\textcolor[HTML]{8F5760}{\pi'}(s)) \geq  q_{\textcolor[HTML]{8F5760}{\pi}}(s,\textcolor[HTML]{8F5760}{\pi}(s)) \text{ for all } s \in \mathcal{S} \rightarrow \textcolor[HTML]{8F5760}{\pi'} \geq \textcolor[HTML]{8F5760}{\pi}
$$
Strictly better:
$$
q_{\textcolor[HTML]{8F5760}{\pi}}(s,\textcolor[HTML]{8F5760}{\pi'}(s)) >  q_{\textcolor[HTML]{8F5760}{\pi}}(s,\textcolor[HTML]{8F5760}{\pi}(s)) \text{ for at least one } s \in \mathcal{S} \rightarrow \textcolor[HTML]{8F5760}{\pi'} > \textcolor[HTML]{8F5760}{\pi}
$$





\subsection{Policy Iteration (Control)}\label{policy-iteration}


Policy iteration is the process of alternating between \textcolor{blue}{\uline{policy evaluation}} and \textcolor{blue}{\uline{policy improvement}}, which can be illustrated by the following figure:


\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.9\columnwidth]{images/policy-iteration.png}
    \caption{Illustration of policy iteration}
    \label{fig:policy-iteration}
\end{figure}

Briefly speaking, we initially take a random policy $\pi$, then compute a state-value function $v_{\pi}$ and use $v_{\pi}$ to compute $q_{\pi}$. After that, we select the new greedy policy $\pi'(s)$ from $q_{\pi}$:
$$\pi'(s)= \argmax_a q_{\pi}(s,a)$$

Each policy is guaranteed to be an improvement on the last unless the last policy was already optimal.


\begin{tcolorbox}[title={Policy Iteration (using iterative policy evaluation) for estimating $\pi \approx \pi_*$}]

  \textbf{1. Initialization:} $V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A}(s)$ arbitrarily for all $s \in \mathcal{S}$ \\

  \textbf{2. Policy Evaluation:}
  \begin{description}
      \item $\;\;\;$ \textbf{Loop:}
      \begin{description}
        \item $\;\;\;$ $\Delta \leftarrow 0$
        \item $\;\;\;$ \textbf{Loop for each $s \in \mathcal{S}$:}
        \begin{description}
          \item $\;\;\;$ $v \leftarrow V(s)$
          \item $\;\;\;$ $V(s) \leftarrow \sum_{s', r} p(s',r|s,\pi(s)) [r + \gamma V(s')]$
          \item $\;\;\;$ $\Delta \leftarrow \max (\Delta , | v - V(s) |)$
        \end{description}
      \end{description}
      \textbf{Until:} $\Delta < \theta$ (a small positive number determining the accuracy of estimation)
  \end{description}

  \textbf{3. Policy Improvement:}
  \begin{description}
      \item $\;\;\;$ $policy–stable \leftarrow true$
      \item $\;\;\;$ \textbf{For each $s \in \mathcal{S}$:}
      \begin{description}
        \item $\;\;\;$ $old–action \leftarrow \pi(s)$
        \item $\;\;\;$ $\pi(s) \leftarrow \argmax_a \sum_{s', r} p(s',r | s,a)[r + \gamma V(s')]$
        \item $\;\;\;$ If $old–action \neq \pi(s)$, then $policy–stable \leftarrow false$
      \end{description}
      \item $\;\;\;$ If $policy–stable$, then stop and return $V \approx v_*$ and $\pi \approx \pi_*$; else go to 2
  \end{description}
\end{tcolorbox}



\subsection{Generalized Policy Iteration (GPI)}\label{generalized-policy-iteration}

\textbf{What:} We use the term generalized policy iteration (GPI) to refer to the \uline{general idea of letting \textcolor{blue}{policy evaluation} and \textcolor{blue}{policy improvement} processes interact}, independent of the granularity and other details of the two processes.

\textbf{Why:} Almost all reinforcement learning methods are well described as GPI.


\subsection{Value Iteration}\label{value-iteration}

\textbf{What:} a special case of generalized policy iteration.

\textbf{Why:} It allows us to \uline{\textbf{combine} \textcolor{blue}{policy evaluation} and \textcolor{blue}{policy improvement} into a single update}.


\begin{tcolorbox}[title={Value Iteration, for estimating  $\pi \approx \pi_*$}]

  \textbf{Algorithm parameters:}
  \begin{itemize}
    \item $\theta > 0 \leftarrow$ small threshold determining accuracy of estimation
  \end{itemize}

  \textbf{Initialize:}
  \begin{itemize}
    \item $V(s)$, for all $s \in \mathcal{S}^+$ arbitrarily
    \item $V(terminal) = 0$
  \end{itemize}


  \textbf{Loop:}
  \begin{description}
      \item $\;\;\;$ $\Delta \leftarrow 0$
      \item $\;\;\;$ \textbf{Loop for each $s \in \mathcal{S}$:}
      \begin{description}
        \item $\;\;\;$ $v \leftarrow V(s)$
        \item $\;\;\;$ $V(s) \leftarrow \max_a \sum_{s', r} p(s',r|s,a) [r + \gamma V(s')]$
        \item $\;\;\;$ $\Delta \leftarrow \max (\Delta , | v - V(s) |)$
      \end{description}
  \end{description}
  \textbf{Until:} $\Delta < \theta$ (a small positive number) \\

  \textbf{Output:} a deterministic policy, $\pi \approx \pi_*$, such that:
  \begin{description}
      \item $\;\;\;$ $\pi(s) = \argmax_a \sum_{s',r} p(s',r|s,a)[r+\gamma V(s')]$
  \end{description}
\end{tcolorbox}


We do not run policy evaluation to completion. We perform just one sweep over all the states. After that, we greedify again. We can write this as an update rule $V(s) \leftarrow \max_a \sum_{s', r} p(s',r|s,a) [r + \gamma V(s')]$ which applies directly to the state-value function. The \uline{update does not reference any specific policy, hence the name \textbf{value iteration}}.

\begin{itemize}
  \item \textcolor{Orange}{Instead of updating the value according to a fixed falsey}, \textcolor{blue}{we update using the action that maximizes the current value estimate}.
\end{itemize}



\subsection{Asynchronous Dynamic Programming}\label{asynchronous-dynamic-programming}

\textbf{What:} a special case of generalized policy iteration.

Asynchronous dynamic programming methods give us freedom to update states in any order, they do not perform systematic sweeps.

Asynchronous algorithms can propagate value information quickly through \textbf{selective updates}. This \uline{can sometimes be more efficient than a systematic sweep}.



\subsection{Efficiency of Dynamic Programming}\label{efficiency-of-dynamic-programming}

The \uline{\textbf{key insight of dynamic programming} is that we do not have to treat the evaluation of each state as a separate problem. We can use the other value estimates we have already worked so hard to compute}.

The process of \uline{using the value estimates of successor states to improve our current value estimate is known as \textbf{bootstrapping}}. This \textcolor{Green}{can be much more efficient than (e.g. a Monte Carlo) method that estimates each value independently}.

\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.8\columnwidth]{images/bootstrapping.png}
    \caption{Illustration of bootstrapping in Dynamic Programming}
    \label{fig:bootstrapping}
\end{figure}























% =============== CHAPTER 5 - Monte Carlo Methods ===============



\section{Monte Carlo Methods}\label{monte-carlo-methods}

\begin{tcolorbox}[colback=yellow!5,colframe=yellow!75!black,title=NOTE]
  To ensure that well-defined returns are available, here we define Monte
  Carlo methods only for \textbf{\colorbox{Yellow}{episodic tasks}}.
  \begin{itemize}
    \item That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected.
    \item Only on the completion of an episode are value estimates and policies changed.
  \end{itemize}
\end{tcolorbox}

\textbf{What:} sample-based methods for solving reinforcement learning problem.
\begin{itemize}
  \item \textbf{estimating} value functions \(\rightarrow\) \textbf{discovering}
  optimal policies.
\end{itemize}

\textbf{How:} based on \textbf{averaging} \uline{sample returns} \textbf{for
each} \uline{state--action pair}.

\textbf{Pros and cons:}
\begin{itemize}
  \item \textbf{+} Don't require a model (direct access to the environment dynamics) $\rightarrow$ Monte Carlo methods learn directly from interaction.
  \item \textbf{+} Conceptually simple
  \item \textbf{-} Not well suited for step-by-step incremental computation
\end{itemize}

\textbf{Base:}

\begin{itemize}
\item
  Unlike previously, we \textbf{don't assume complete knowledge of the
  environment}.

  \begin{itemize}
  \item
    Unknown transaction dynamics (T) and reward function (r)
  \end{itemize}
\item
  Require only \emph{experience} --- \textbf{sample} sequences of
  \textbf{states, actions, and rewards} from actual or simulated
  interaction with an environment.
\item
  There are \textbf{multiple states}, \emph{each acting like a different
  bandit problem} (like an associative-search or contextual bandit) and
  the different bandit problems are interrelated.
\end{itemize}

\textbf{Terms:}

\begin{itemize}
\item
  The term \textbf{``Monte Carlo''} is often used more broadly for any
  estimation method whose operation involves a significant random
  component.

  \begin{itemize}
  \item
    \textbf{HERE:} Here we use it specifically for methods based on
    averaging complete returns.
  \end{itemize}
\end{itemize}






\subsection{Monte Carlo Policy Evaluation (Prediction)}\label{monte-carlo-prediction}

\textbf{What:} \uline{\textbf{\textcolor{blue}{Monte Carlo methods}} for \textbf{Policy Evaluation}} (estimating value function $v_\pi$ for a specific policy $\pi$).


\textbf{Implications of MC learning:}

\begin{itemize}
  \item \textbf{learns directly from experience} $\rightarrow$ \textcolor{Green}{no need to keep a large model of the environment}.
  \item can \textbf{estimate the value of an individual state independently of the values of any other states}.
  \item \textbf{computation needed to update the value of each state} along the way \textcolor{Green}{doesn't depend in any way on the size of the MDP}.
  \begin{itemize}
    \item Rather, it \textcolor{Red}{depends on the length of the episode}.
  \end{itemize}
\end{itemize}



The \textbf{first-visit MC method} (focus of this chapter) estimates
\(v_\pi(s)\) as the average of the returns following first visits to s,
whereas the \textbf{every-visit MC method} averages the returns
following all visits to s.



\begin{tcolorbox}[title={First-visit MC prediction, for estimating $V \approx v_\pi$}]

  \textbf{Input:} a policy $\pi$ to be evaluated

  \textbf{Initialize:}
  \begin{itemize}
    \item $V(s) \in \mathbb{R}$, for all $s \in \mathcal{S}$ arbitrarily
    \item $Returns(s) \leftarrow$ an empty list, for all $s \in \mathcal{S}$
  \end{itemize}

  \textbf{Loop forever (for each episode):}
  \begin{description}
      \item $\;\;\;$ Generate an episode following $\pi$
      \item $\;\;\;$ $G \leftarrow 0$
      \item $\;\;\;$ \textbf{Loop for each step of episode, $t=T-1,T-2,...,0$:}
      \begin{description}
        \item $\;\;\;$ $G \leftarrow \gamma G + R_{t+1}$
        \item $\;\;\;$ \textbf{Unless $S_t$ appears in $S_0, S_1, ..., S_{t-1}:$}
        \begin{description}
          \item $\;\;\;$ Append $G$ to $Returns(S_t)$
          \item $\;\;\;$ $V(S_t) \leftarrow average(Returns(S_t))$
        \end{description}
      \end{description}
  \end{description}
\end{tcolorbox}

\begin{itemize}
  \item We can repeat the whole process over many episodes (loop forever) and \uline{eventually learn a good estimate for the value function}.
  \item \textbf{We can avoid keeping all the sampled returns in a list:} We can incrementally update the sample average estimated using the formula:
  \begin{gather*}
    Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n] \\
    NewEstimate = OldEstimate + StepSize[Target - OldEstimate]
  \end{gather*}
\end{itemize}









\subsection{Monte Carlo Estimation of Action Values}\label{monte-carlo-estimation-of-action-values}


\textbf{What:} Using \uline{\textbf{\textcolor{blue}{Monte Carlo methods}} to learn \textbf{Action-Value functions $q_\pi$}} (instead of state-values).

\begin{itemize}
  \item Recall that we \textbf{learned state-values by} \uline{averaging sample returns from that state}.
  \item Similarny we \textbf{learn action-values by} \uline{averaging sample returns from that state, action pair}.
\end{itemize}


\textbf{Why:} Action values are useful for learning a policy. They allow us to \uline{compare different actions in the same state}. Then, we can switch to a better action if one is available.







\subsection{Monte Carlo Control}\label{monte-carlo-control}

\textbf{What:} Using \uline{\textbf{\textcolor{blue}{Monte Carlo methods}} to \textbf{approximate optimal policies}}. Namely Monte Carlo methods to implement a Generalized Policy Iteration (GPI) algorithm (\textcolor{blue}{policy evaluation} and \textcolor{blue}{policy improvement} processes interact).

\textbf{How:}

\begin{itemize}
  \item For the \textcolor{blue}{policy improvement step}, we can make the policy \uline{greedy with respect to the agent's current action value estimates}.
  \item For the \textcolor{blue}{policy evaluation step}, we will use a \uline{Monte Carlo method to estimate the action values}.
  \item For Monte Carlo policy iteration it is natural to \textbf{alternate between} \textcolor{blue}{evaluation} and \textcolor{blue}{improvement} on an \uline{episode-by-episode basis}.
  \begin{itemize}
    \item \textbf{After each episode},
    \begin{enumerate}
      \item the \textbf{observed returns} are \uline{used for \textcolor{blue}{policy evaluation}}, and
      \item then the \uline{\textcolor{blue}{policy is improved} at all the states visited in the episode}.
    \end{enumerate}
  \end{itemize}
\end{itemize}


\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=0.8\columnwidth]{images/monte-carlo-policy-iteration.png}
    \caption{Process of Monte Carlo General Policy Iteration.}
    \label{fig:monte-carlo-policy-iteration}
\end{figure}

 A complete simple algorithm along these lines, which we call Monte Carlo ES, for Monte Carlo with Exploring Starts, is given in pseudocode below.



\begin{tcolorbox}[title={Monte Carlo ES (Exploring Starts), for estimating $\pi \approx \pi_*$}]

\textbf{Initialize:}
\begin{itemize}
 \item $\pi(s) \in \mathcal{A}(s)$, for all $s \in \mathcal{S}$ arbitrarily
 \item $Q(s,a) \in \mathbb{R}$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$ arbitrarily
\end{itemize}

\textbf{Loop forever (for each episode):}
\begin{description}
   \item $\;\;\;$ Choose $S_0 \in \mathcal{S}, A_0 \in \mathcal{A}(\mathcal{S}_0)$ randomly such that all pairs have probability $> 0$
   \item $\;\;\;$ Generate an episode from $S_0$, $A_0$, following $\pi$
   \item $\;\;\;$ $G \leftarrow 0$
   \item $\;\;\;$ \textbf{Loop for each step of episode, $t=T-1,T-2,...,0$:}
   \begin{description}
     \item $\;\;\;$ $G \leftarrow \gamma G + R_{t+1}$
     \item $\;\;\;$ \textbf{Unless $S_t, A_t$ appears in $S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}:$}
     \begin{description}
       \item $\;\;\;$ Append $G$ to $Returns(S_t, A_t)$
       \item $\;\;\;$ $Q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))$
       \item $\;\;\;$ $\pi(S_t) \leftarrow \argmax_a Q(S_t, a)$
     \end{description}
   \end{description}
\end{description}
\end{tcolorbox}

\begin{itemize}
  \item \textbf{Exploring start:} For policy evaluation to work for action values, we must \uline{assure continual exploration}. One way to do this is by specifying that the \uline{episodes start in a state–action pair, and that every pair has a nonzero probability of being selected as the start}. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.
\end{itemize}








\subsection{Monte Carlo Control without Exploring Starts}\label{monte-carlo-control-without-exploring-starts}

\textbf{What:} Alternative ways (to Exploring Starts) for performing exploration. Here we use $\epsilon$-Soft policies.

\textbf{Why:} It can be difficult to randomly sample an initial State action pair.

\begin{itemize}
  \item e.g. For example, how would you randomly sample the initial State action pair for a self-driving car? How could we ensure the agent can start in all possible States? We would need to put the car in many different configurations in the middle of a busy freeway. This would be dangerous and impractical.
\end{itemize}



\textbf{\textcolor{blue}{$\epsilon$-Soft policies}:} take each action with probability at least Epsilon over the number of actions ($\epsilon/|\mathcal{A}|$).

\begin{figure}[H]
  \centering  % Remember to centre the figure
    \includegraphics[width=1.0\columnwidth]{images/epsilon-soft-policies.png}
    \caption{$\epsilon$-greedy policies are a subset of $\epsilon$-soft policies}
    \label{fig:monte-carlo-policy-iteration}
\end{figure}

However, if our policy always gives at least Epsilon probability to each action, it's \textcolor{Red}{impossible to converge to a deterministic optimal policy}
$$
\pi_* > Optimal\;\epsilon–soft
$$


We will later discuss how \textcolor{Green}{we can learn the optimal policy using a different method called \textbf{Q-learning}}.


\begin{tcolorbox}[title={On-policy first-visit MC control (for $\epsilon$-soft policies), estimates $\pi \approx \pi_*$}]

\textbf{Algorithm parameter:} small $\epsilon$ > 0

\textbf{Initialize:}
\begin{itemize}
 \item $\pi(s) \leftarrow$ an arbitrary $\epsilon$-soft policy
 \item $Q(s,a) \in \mathbb{R}$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$ arbitrarily
 \item $Returns(s,a) \leftarrow$ an empty list, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$
\end{itemize}

\textbf{Repeat forever (for each episode):}
\begin{description}
   \item $\;\;\;$ Generate an episode from $S_0$, $A_0$, following $\pi$:
   \item $\;\;\;$ $G \leftarrow 0$
   \item $\;\;\;$ \textbf{Loop for each step of episode, $t=T-1,T-2,...,0$:}
   \begin{description}
     \item $\;\;\;$ $G \leftarrow \gamma G + R_{t+1}$
     \item $\;\;\;$ \textbf{Unless $S_t, A_t$ appears in $S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}:$}
     \begin{description}
       \item $\;\;\;$ Append $G$ to $Returns(S_t, A_t)$
       \item $\;\;\;$ $Q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))$
       \item $\;\;\;$ $A^* \leftarrow \argmax_a Q(S_t,a)$     (with ties broken arbitrarily)
       \item $\;\;\;$ For all $a \in \mathcal{A}(S_t)$
       $$
       \pi(a|S_t)=
        \begin{cases}
           1-\epsilon + \epsilon / |\mathcal{A}(S_t)| & \text{if } a = A^* \\
          \epsilon / |\mathcal{A}(S_t)| & \text{if } a \neq A^*
        \end{cases}
       $$
     \end{description}
   \end{description}
\end{description}
\end{tcolorbox}






\subsection{Off-policy Prediction via Importance Sampling}\label{off-policy-prediction-via-importance-sampling}

\textbf{What:} Importance sampling allowing us to do Off-Policy learning. More precisely, we use the important sampling ratios to correct the returns generated by a behavior policy and we modified the on-policy Monte Carlo prediction algorithm for off-policy learning.

\textbf{Why:} Off-Policy learning allows us to learn optimal policy from suboptimal behaviour. This way we can \textcolor{Green}{learn an optimal policy but still maintain exploration}.

\textbf{On-Policy vs. Off-Policy:}
\begin{itemize}
  \item \textbf{\textcolor{Blue}{On-Policy}:} \textcolor{blue}{improve} and \textcolor{blue}{evaluate} the \uline{policy being used to select actions}.
  \item \textbf{\textcolor{Blue}{Off-Policy}:} \textcolor{blue}{improve} and \textcolor{blue}{evaluate} a \uline{different policy (\textcolor{Blue}{behavio policy $b(s|a)$}) from the one used to select actions (\textcolor{Blue}{target policy $\pi(s|a)$})}.
  \begin{itemize}
    \item \textcolor{Blue}{behavio policy $b(s|a)$} is generally an \uline{exploratory policy}
    \item One key rule of off policy learning is that the behavior policy must cover the target policy
    $$
    \text{If } \pi(a|s) > 0 \text{ then also } b(a|s) > 0
    $$
  \end{itemize}
  \item Learning \textcolor{Blue}{on-policy} or \textcolor{Blue}{off-policy} \textcolor{Orange}{may perform differently in control depending on the task}.
\end{itemize}


\textbf{Importance Sampling:} A way of \uline{estimating the expected value of a distribution using samples from a different distribution}.
\begin{itemize}
  \item Derivation of Importance Sampling – Consider:
  \begin{itemize}
    \item Random variable $x$ is being sampled from a probability distribution $b$
    $$
    \text{Sample: } x \sim b
    $$
    \item We want to estimate the expected value of $x$, but with respect to the target distribution $\pi$.
    $$
    \text{Estimate: } \mathbb{E}_\pi[X]
    $$

    \item \textcolor{Red}{Because $x$ is drawn from $b$, we cannot simply use the sample average to compute the expectation under $\pi$.}

    \item We can use importance sampling to correct the expectation:
      \begin{align*}
      \mathbb{E}_\pi[X] &=
      \sum_{x \in X} x \frac{\pi(x)}{b(x)} b(x) \\
      &= \sum_{x \in X} x \underbrace{\rho(x)}_\text{importance sampling ratio}b(x) \\
      &= \mathbb{E}_b [X\rho(x)] \\
      \end{align*}
    \item We can then use importance sampling to \textbf{estimate the expectation from data:} We just need to compute a \uline{weighted sample average with the importance sampling ratio as the weightings}:
      \begin{align*}
      \mathbb{E}_b [X\rho(x)] &=
      \sum_{x \in X} x \rho(x) b(x) \\
      &\approx \frac{1}{n} \sum_{i=1}^n x_i \rho(x_i)
      \end{align*}
    \item \textcolor{Green}{We can now estimate the expected value of $x$ under distribution $\pi$ by using the sample average, the samples drawn from distribution $b$.}

    \begin{figure}[H]
      \centering  % Remember to centre the figure
        \includegraphics[width=0.7\columnwidth]{images/importance-sampling-eg.png}
        \caption{Example importance sampling (samples 1, 3 \& 1)}
        \label{fig:importance-sampling-eg}
    \end{figure}
    \begin{itemize}
      \item With samples just from $b$, we managed to get a pretty good estimate of the expectation under $\pi$.
    \end{itemize}
  \end{itemize}
\end{itemize}


\textbf{Off-policy Prediction via Importance Sampling:}
\begin{itemize}
  \item The expectation of the return under $\pi$, sampling from $b$:
  \begin{gather*}
  V_\pi(s) = \mathbb{E}_b [\rho G_t | S_t = s] \\ \\
  \textbf{Where:} \\
  \text{Rho: } \rho = \frac{\mathbb{P}(\text{trajectory under $\pi$})}{\mathbb{P}(\text{trajectory under $b$})} \\
  = \prod_{k=t}^{T-1} \frac{\pi(A_k | S_k)}{b(A_k | S_k)} \\ \\
  \textbf{Where:} \\
  \mathbb{P}(\text{trajectory under $b$}) = \prod_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k) \\
  \mathbb{P}(\text{trajectory under $\pi$}) = \prod_{k=t}^{T-1} \pi(A_k | S_k) p(S_{k+1} | S_k, A_k) \\
\end{gather*}
\end{itemize}




\textbf{Incremental Implementation:}

Monte Carlo prediction methods can be implemented incrementally, on an episode-by-episode basis, using average returns.


\begin{tcolorbox}[title={Off-policy MC prediction (policy evaluation) for estimating $Q \approx q_\pi$}]


\textbf{Input:} an arbitrary target policy $\pi$

\textbf{Initialize, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$:}
\begin{itemize}
 \item $Q(s,a) \in \mathbb{R}$ (arbitrarily)
 \item $C(s,a) \leftarrow 0$
\end{itemize}

\textbf{Loop forever (for each episode):}
\begin{description}
   \item $\;\;\;$ $b$ any policy with coverage of $\pi$
   \item $\;\;\;$ Generate an episode following $b$
   \item $\;\;\;$ $G \leftarrow 0$
   \item $\;\;\;$ $W \leftarrow 1$
   \item $\;\;\;$ \textbf{Loop for each step of episode, $t=T-1,T-2,...,0$, while $W \neq 0$:}
   \begin{description}
     \item $\;\;\;$ $G \leftarrow \gamma G + R_{t+1}$
     \item $\;\;\;$ $C(S_t,A_t) \leftarrow C(S_t,A_t) + W$
     \item $\;\;\;$ $Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{W}{C(S_t,A_t)} [G - Q(S_t,A_t)]$
     \item $\;\;\;$ $W \leftarrow W\frac{\pi(A_k | S_k)}{b(A_k | S_k)}$
   \end{description}
\end{description}
\end{tcolorbox}

\begin{itemize}
  \item $W:$ accumulated product of important sampling ratios on each time step of the episode.
  \item We compute Rho $\rho$ from $t$ to $T-1$ incrementally ($W = \rho_{t:T-1}$).
  $$
  W_{t+1} \leftarrow W_t \rho_t
  $$
  \begin{itemize}
    \item We can compute this recursively without having to store all past values of Rho.
  \end{itemize}
\end{itemize}













\subsection{Off-policy Monte Carlo Control}\label{off-policy-monte-carlo-control}


\begin{tcolorbox}[title={Off-policy MC control for estimating $\pi \approx \pi_*$}]

\textbf{Initialize, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$:}
\begin{itemize}
 \item $Q(s,a) \in \mathbb{R}$ (arbitrarily)
 \item $C(s,a) \leftarrow 0$
 \item $\pi(s) \leftarrow \argmax_a Q(s,a)$ (with ties broken consistently)
\end{itemize}

\textbf{Loop forever (for each episode):}
\begin{description}
   \item $\;\;\;$ $b$ any soft policy
   \item $\;\;\;$ Generate an episode using $b$
   \item $\;\;\;$ $G \leftarrow 0$
   \item $\;\;\;$ $W \leftarrow 1$
   \item $\;\;\;$ \textbf{Loop for each step of episode, $t=T-1,T-2,...,0$:}
   \begin{description}
     \item $\;\;\;$ $G \leftarrow \gamma G + R_{t+1}$
     \item $\;\;\;$ $C(S_t,A_t) \leftarrow C(S_t,A_t) + W$
     \item $\;\;\;$ $Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{W}{C(S_t,A_t)} [G - Q(S_t,A_t)]$
     \item $\;\;\;$ $\pi(S_t) \leftarrow \argmax_a Q(S_t,a)$ (with ties broken consistently)
     \item $\;\;\;$ If $A_t \neq \pi(S_t)$ then exit inner Loop (proceed to next episode)
     \item $\;\;\;$ $W \leftarrow W\frac{1}{b(A_t | S_t)}$
   \end{description}
\end{description}
\end{tcolorbox}















% =============== CHAPTER 6 - Temporal-Difference Learning ===============



\section{Temporal-Difference (TD) Learning}\label{temporal-difference-learning}


\begin{tcolorbox}[colback=green!5,colframe=green!75!black]
  One of the most fundamental concepts in reinforcement learning!
\end{tcolorbox}


\textbf{What:} class of \uline{model-free} reinforcement learning methods which learn by \uline{bootstrapping} from the current estimate of the value function. These methods are a \uline{combination of \textcolor{blue}{Monte Carlo (MC)} ideas and \textcolor{blue}{dynamic programming (DP)} ideas}.

\textbf{How:}
\begin{itemize}
  \item Like \textcolor{blue}{MC}, TD methods can \uline{learn directly from raw experience without a model of the environment’s dynamics}.
  \item Like \textcolor{blue}{DP}, TD methods \uline{update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap)}.
\end{itemize}

For the control problem (finding an optimal policy), DP, TD, and MC methods \uline{all use some variation of \textbf{generalized policy iteration (GPI)}}. The differences in the methods are primarily differences in their approaches to the prediction problem.










\subsection{TD Prediction}\label{td-prediction}

\textbf{TD vs. MC}
\begin{itemize}
  \item \textbf{Both} \uline{use experience to solve the prediction problem}.
  \item \textbf{\textcolor{blue}{MC methods}} \uline{wait until the end of the episode to determine the increment to $V(S_t)$} (only then is $G_t$ known).
  \item \textbf{\textcolor{blue}{TD methods}} \uline{wait only until the next time step}. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V (S_{t+1})$.
\end{itemize}


\textbf{We can use these recursive formula to incrementally update our estimated value:}

First recall:
\begin{itemize}
  \item \textbf{$G_t$ (recursively):}
  $$
  G_t \dot{=} R_{t+1} + \gamma G_{t+1}
  $$
  \item \textbf{$v_\pi(s)$ (recursively):}
    \begin{align*}
    v_\pi(s) &= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s] \\
    &= R_{t+1} + \gamma v_\pi(S_{t+1})
    \end{align*}
\end{itemize}

Incremental Monte Carlo update rule (Monte Carlo estimate without saving lists of returns):
$$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$$
\begin{itemize}
  \item We can replace $G_t$ with the \uline{reward + the estimate of the return in the next state} (so we \textcolor{Green}{don't have to wait until the end of the episode}, but we still have to wait to the next step):
  $$V(S_t) \leftarrow V(S_t) + \alpha \underbrace{[\underbrace{R_{t+1} + \gamma V(S_{t+1})}_\text{t target} - V(S_t)]}_\text{TD-error $\delta_t$}$$
  \item TD updates the value of one state towards its own estimate of the value in the next state. As the estimated value for the next state improves, so does our target.
  \begin{itemize}
    \item \textcolor{Orange}{In fact, we've done something like this before in dynamic programming. In DP, we update $V(S_t)$ toward the value of all possible next states. The primary difference is in DP, we use an expectation over all possible next states. We needed a model of the environment to compute this expectation, in TD we only need the next state. We can get that directly from the environment without a model.}
  \end{itemize}

  We can get the next state directly from the environment without a model, \textbf{1-Step TD}:
  \begin{itemize}
    \item Think of time $t+1$ as the current time step and time $t$ as the previous time step.
    \item Simply store the state from the previous time step in order to make our TD updates: $s_t \leftarrow s_{t+1}$
    \item Only then can we update the value of the previous state.
    \begin{figure}[H]
      \centering  % Remember to centre the figure
        \includegraphics[width=0.7\columnwidth]{images/1-step-td.png}
        \caption{Illustration of 1-Step TD, TD(0)}
        \label{fig:1-step-td}
    \end{figure}
  \end{itemize}
\end{itemize}

\textbf{TD(0) a.k.a 1-Step TD}

So the simplest TD method (TD(0)) makes the update
$$V(S_t) \leftarrow V(S_t) + \alpha \underbrace{[\underbrace{R_{t+1} + \gamma V(S_{t+1})}_\text{t target} - V(S_t)]}_\text{TD-error $\delta_t$}$$
immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$.

It is a special case of the $TD(\lambda)$ and $n$-step TD methods.

The below pseudocode specifies TD(0) completely in procedural form.



\begin{tcolorbox}[title={Tabular TD(0) for estimating $v_\pi$}]

\textbf{Input:} the policy $\pi$ to be evaluated

\textbf{Algorithm parameter:} step size $\alpha \in (0,1])$

\textbf{Initialize:} $V(s)$ for all $s \in \mathcal{S}^+$ arbitrarily except that $V (terminal) = 0$ \\

\textbf{Loop for each episode:}
\begin{description}
   \item $\;\;\;$ Initialize $S$
   \item $\;\;\;$ \textbf{Loop for each step of episode:}
   \begin{description}
     \item $\;\;\;$ $A \leftarrow$ action given by $\pi$ for $S$
     \item $\;\;\;$ Take action $A$ observe $R, S'$
     \item $\;\;\;$ $V(S) \leftarrow V(S) + \alpha [R + \gamma V(S') - V(S)]$
     \item $\;\;\;$ $S \leftarrow S'$
   \end{description}
   \item $\;\;\;$ \textbf{until $S$ is terminal}
\end{description}
\end{tcolorbox}






\subsection{Rich Sutton: The Importance of TD Learning}\label{the-importance-of-td-learning}


\textbf{\textcolor{blue}{Learning to predict (Prediction Learning}) is the single most scalable kind of learning.}

\begin{itemize}
  \item It is the \uline{unsupervised supervised learning}.
  \begin{itemize}
    \item We have a target (just by waiting)
    \item Yet no human labeling is needed!
  \end{itemize}
  \item Prediction learning is \uline{\textbf{scalable} model-free learning}.
\end{itemize}


\textbf{\textcolor{blue}{TD learning} is a method for \textcolor{blue}{learning to predict} from another, later, learned prediction.}

\begin{itemize}
  \item i.e. \textit{learnig a guess from guess}
  \item The TD error is the difference between the two predictions, the \textit{temporal difference}
  \item Otherwise TD learning is the same as supervised-learning, backpropagating the error
\end{itemize}


\textbf{\textcolor{blue}{TD learning} benefits and insights.}

\begin{itemize}
  \item Relevant only on \uline{\textit{multi-step} prediction problems} (everything other than the classical supervised learning setup)
  \begin{itemize}
    \item with information about it possibly revealed on each step
  \end{itemize}
  \item It is learning specialized for general, multi-step prediction, which may be key to perception, meaning, and modeling of the world
  \item It takes advantage of the \textbf{state property}
  \begin{itemize}
    \item which makes it \uline{fast, data efficient}
    \item which also makes it \uline{asymptotically biased}
  \end{itemize}
  \item It is \uline{computationally congenial}
  \item \textbf{Widely used in RL} to predict future rewards (value functions)
  \item \textbf{Key to} $Q$-learning, Sarsa, TD($\lambda$), Deep Q networks, TD-Gammon actor-critic methods, Samuel's checker player
  \begin{itemize}
    \item but not AlphaGo, helicopter autopilots, pure policy-based methods
  \end{itemize}
  \item Appears to be how brain reward systems work
  \item Can be used to predict any signal, not just rewards
\end{itemize}











\subsection{Advantages of TD Prediction Methods}\label{advantages-of-td-prediction-methods}


\begin{itemize}
  \item Unlike dynamic programming, TD methods \uline{do not require a model of the environment}.
  \item Unlike Monte Carlo, TD methods can \uline{learn online} (update the values on every step) without waiting to know the final outcome. Bootstrapping allows us to update the estimates based on other estimates.
  \item \uline{Converge faster than MC methods}.
\end{itemize}







\subsection{Optimality of TD(0)}\label{optimality-of-td(0)}

Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter, $\alpha$, as long as $\alpha$ is chosen to be sufficiently small.

The constant-$\alpha$ MC method also converges deterministically under the same conditions, but to a different answer.
\begin{itemize}
  \item Under batch training, constant-$\alpha$ MC converges to values, V(s), that are sample averages of the actual returns experienced after visiting each state s.
\end{itemize}

These are optimal estimates in the sense that they minimize the mean-squared error from the actual returns in the training set. In this sense it is surprising that the batch TD method was able to perform better according to the root mean-squared error measure shown in Figure \ref{fig:batch-training-td-vs-mc} below.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\columnwidth]{images/batch-training-td-vs-mc.png}
    \caption{Performance of TD(0) and constant-$\alpha$ MC under batch training on the random walk task}
    \label{fig:batch-training-td-vs-mc}
\end{figure}

The answer is that the Monte Carlo method is optimal only in a limited way, and
that \uline{TD is optimal in a way that is more relevant to predicting returns}.

On tasks with large state spaces, TD methods may be the only feasible way of approximating the certainty-equivalence solution.










\subsection{Sarsa: On-policy TD Control}\label{sarsa}

\textbf{What:} algorithm that perfrms on-policy TD control by learning an action-value function $q_\pi$.

The \textbf{\textcolor{blue}{SARSA}} acronym describes the data used in the updates:
\begin{itemize}
  \item \textbf{\textcolor{blue}{S}}tate, \textbf{\textcolor{blue}{A}}ction, \textbf{\textcolor{blue}{R}}eward, next \textbf{\textcolor{blue}{S}}tate, next \textbf{\textcolor{blue}{A}}ction.
  \item $S_t \;, \;\;\; A_t \;, \;\;\; R_{t+1} \;, \;\;\; S_{t+1} \;, \;\;\; A_{t+1}$
\end{itemize}


\textbf{\textcolor{blue}{Sarsa algorithm:}}

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
$$

The general form of the Sarsa control algorithm is given in the box below.

\begin{tcolorbox}[title={Sarsa (on-policy TD control) for estimating $Q \approx q_*$}]

\textbf{Algorithm parameters:} $\alpha \in (0,1])$ \& small $\epsilon > 0$

\textbf{Initialize:} $Q(s, a)$ for all $s \in \mathcal{S}^+, a \in \mathcal{A}(s)$ arbitrarily except that $Q(terminal, \cdot) = 0$ \\

\textbf{Loop for each episode:}
\begin{description}
   \item $\;\;\;$ Initialize $S$
   \item $\;\;\;$ Choose $A$ from $S$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)
   \item $\;\;\;$ \textbf{Loop for each step of episode:}
   \begin{description}
     \item $\;\;\;$ Take action $A$ observe $R, S'$
     \item $\;\;\;$ Choose $A'$ from $S'$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)
     \item $\;\;\;$ $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S', A') - Q(S, A)]$
     \item $\;\;\;$ $S \leftarrow S'; A \leftarrow A'$
   \end{description}
   \item $\;\;\;$ \textbf{until $S$ is terminal}
\end{description}
\end{tcolorbox}

In Sarsa, the agent needs to know its next state action pair before updating its value estimates. That means it \uline{has to commit to its next action before the update}.

\textcolor{Green}{Sarsa converges with probability 1 to an optimal policy} and action-value function \textbf{as long as} \uline{all state-action pairs are visited an infinite number of times} and the \uline{policy converges in the limit to the greedy policy} (which can be arranged, for example, with "$\epsilon$-greedy policies by setting " = 1/t).














\subsection{Q-learning: Off-policy TD Control}\label{q-learning}

\textbf{What:} algorithm that perfrms off-policy TD control by learning an action-value function $q_\pi$.

\textbf{\textcolor{blue}{Q-learning algorithm:}}

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t))
$$


\textcolor{blue}{\textbf{Sarsa}} vs. \textcolor{blue}{\textbf{Q-learning}}:

\begin{itemize}
  \item \textcolor{blue}{\textbf{Sarsa}} is a \uline{sample-based version of \textcolor{blue}{policy iteration} which uses \textcolor{blue}{Bellman equation} for action values, that each depend on a fixed policy}.
  \item \textcolor{blue}{\textbf{Q-learning}} is a \uline{sample-based version of \textcolor{blue}{value iteration} which iteratively applies the \textcolor{blue}{Bellman's Optimality Equation}}.
  \begin{figure}[H]
    \centering
      \includegraphics[width=0.8\columnwidth]{images/bellman-equation-sarsa-qlearning.png}
      \label{fig:bellman-equation-sarsa-qlearning}
  \end{figure}
  \item The optimality equations enable \textcolor{blue}{\textbf{Q-learning}} to \textcolor{Green}{directly learn Q-star instead of switching between policy improvement and policy evaluation steps}.
\end{itemize}


\begin{tcolorbox}[title={Q-learning (off-policy TD control) for estimating $\pi \approx \pi_*$}]

\textbf{Algorithm parameters:} step size $\alpha \in (0,1])$ \& small $\epsilon > 0$

\textbf{Initialize:} $Q(s, a)$ for all $s \in \mathcal{S}^+, a \in \mathcal{A}(s)$ arbitrarily except that $Q(terminal, \cdot) = 0$ \\

\textbf{Loop for each episode:}
\begin{description}
   \item $\;\;\;$ Initialize $S$
   \item $\;\;\;$ \textbf{Loop for each step of episode:}
   \begin{description}
     \item $\;\;\;$ Choose $A$ from $S$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)
     \item $\;\;\;$ Take action $A$ observe $R, S'$
     \item $\;\;\;$ $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_a Q(S', a) - Q(S, A)]$
     \item $\;\;\;$ $S \leftarrow S'$
   \end{description}
   \item $\;\;\;$ \textbf{until $S$ is terminal}
\end{description}
\end{tcolorbox}


\bigskip

\textbf{How is Q-learning off-policy?}
\begin{itemize}
  \item Since Q-learning \uline{learns about the best action it could possibly take rather than the actions it actually takes}, it is learning off-policy.
  \item \textbf{Why no important sampling ratios (with 1-step)?} Because the agent is estimating action values with unknown policy. It does not need important sampling ratios to correct for the difference in action selection. The action value function represents the returns following each action in a given state. The agents target policy represents the probability of taking each action in a given state. Putting these two elements together, the agent can calculate the expected return under its target policy from any given state, in particular, the next state, $S_{t+1}$.

\end{itemize}

\textcolor{Green}{Q-learning also converges to the optimal value function} as long as the aging continues to explore and samples all areas of the state action space.











\subsection{Expected Sarsa}\label{expected-sarsa}

The algorithm is nearly identical to Sarsa, except the TD error uses the expected estimate of the next action value instead of a sample of the next action value.
\begin{itemize}
  \item Sarsa:
  $$
  Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \textcolor{purple}{Q(S_{t+1}, A_{t+1})} - Q(S_t, A_t))
  $$
  \item Expected Sarsa:
  \begin{align*}
    Q(S_t, A_t) &\leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \textcolor{purple}{\mathbb{E}_\pi [Q(S_{t+1}, A_{t+1}) | S_{t+1}]} - Q(S_t, A_t)) \\
    &\leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \textcolor{purple}{\sum_{a'} \pi(a' | S_{t+1}) Q(S_{t+1}, a')} - Q(S_t, A_t))
  \end{align*}
\end{itemize}


\textbf{Motivation:} Sarsa estimates this expectation by sampling the next date from the environment and the next action from its policy. But the agent already knows this policy, so why should it have to sample its next action? Instead, it should just compute the expectation directly.

\textbf{Expected Sarsa vs. Sarsa (Pross \& Cons)}
\begin{itemize}
  \item There's a huge upside to calculating the expectation explicitly: Expected Sarsa has a \textcolor{Green}{more stable update target} than Sarsa.
  \item The lower variance comes with a downside though. Computing the average over next actions becomes \textcolor{red}{more expensive as the number of actions increases}. When there are many actions, computing the average might take a long time, especially since the average has to be computed every time step.
  \item Expected Sarsa is \textcolor{Green}{more robust} than Sarsa \textcolor{Green}{to large step sizes}.
\end{itemize}

Expected Sarsa can do off-policy learning without using importance sampling.

Expected Sarsa with greedy target policy $\equiv (equivalent\;to)$ Q-Learning
\begin{itemize}
  \item[$\rightarrow$] Q-Learning is a special case of Expected Sarsa.
  \item Expected Sarsa can also do off-policy learning without using importance sampling.
\end{itemize}


















\section{$n$-step Bootstrapping}

\begin{tcolorbox}[colback=yellow!5,colframe=yellow!75!black,title=NOTE]
  \textbf{Idea of $n$-step methods} is \textcolor{Orange}{usually used as an \uline{introduction to the algorithmic idea of \textit{eligibility traces} (Chapter 12)}}, which enable bootstrapping over multiple time intervals simultaneously. \textbf{Here} we instead consider the n-step bootstrapping idea on its own.
\end{tcolorbox}


\textbf{What:} methods that \uline{unifies \textcolor{blue}{MC} \& \textcolor{blue}{TD(0)} methods} so that one can shift from one to the other smoothly as needed to meet the demands of a particular task.

\textbf{Motivation:} Neither \textcolor{blue}{MC} methods nor \textcolor{blue}{one-step TD} methods are always the best. The \textcolor{Green}{best methods are often intermediate between the two extremes}.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.9\columnwidth]{images/n-step-methods.png}
    \caption{These $n$-step methods form a spectrum ranging from one-step TD methods to Monte Carlo methods.}
    \label{fig:n-step-methods}
\end{figure}


\textbf{How they work:}
\begin{itemize}
  \item $n$-step TD will perform an update based on the next $n$ rewards, and the estimated value of the corresponding state ($n$ steps ahead).
\end{itemize}

\textbf{Returns of the method on the spectrum:}
\begin{itemize}
  \item \textbf{Monte Carlo} updates the estimate of $v_\pi(S_t)$ is updated in the direction of the \uline{complete return}:
  \begin{gather*}
    G_{t} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... + \gamma^{T-t-1} R_{T} \\ \\
    \textbf{Where:} \\
    T\text{: is the last time step of the episode} \\
  \end{gather*}
  \begin{itemize}
    \item Let us call this quantity the \textit{target} of the update.
  \end{itemize}
  \item All \textbf{$n$-step returns} can be considered \uline{approximations to the full return}, truncated after n steps and then corrected for the remaining missing terms by $V_{t+n-1}(S_{t+n})$.
  \begin{itemize}
    \item Return in 1-step TD method:
    $$
    G_{t:t+1} \doteq R_{t+1} + \gamma V_{t}(S_{t+1})
    $$
    \item Return in 2-step TD method:
    $$
    G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1}(S_{t+2})
    $$
    \item Return in $n$-step TD method:
    $$
    G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+n} + \gamma^{n} V_{t+n-1}(S_{t+n})
    $$
    \item If $t+n \leq T$ (if the $n$-step return extends to or beyond termination), then all the missing terms are taken as zero, and the $n$-step return defined to be equal to the ordinary full return ($G_{t:t+n} \doteq G_t \text{ if } t+n \geq T$).
  \end{itemize}
\end{itemize}


\textbf{$n$-step TD Algorithm:}
\begin{itemize}
  \item No real algorithm can use the $n$-step return until after it has seen $R_{t+n}$ and computed $V_{t+n-1}$. The first time these are available is $t+n$. The natural state-value learning algorithm for using $n$-step returns is thus
  $$
  V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha (G_{t:t+n} - V_{t+n-1}(S_t)), \;\;\;\; 0 \leq t < T,
  $$
  \item while the values of all other states remain unchanged: $V_{t+n}(s) = V_{t+n-1}(s)$, for all $s \neq S_t$.
\end{itemize}
















\section{Planning and Learning with Tabular Methods (Model based RL)}

(Book section 8)

So far, we’ve used off-policy learning to facilitate exploration in a problem that has one goal. But actually you can use off-policy learning to learn how to get to many different goals.




\end{document}
